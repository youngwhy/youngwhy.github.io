---
title: "LeNet"
date: 2020-09-19 23:46:28 -0400
categories: jekyll update
---
LeNet
	Introduction 
-기존 Fully-Connected Neural Network에 대해서, Deep Learning에 가장 적합한 알고리즘 이지만, 필자가 서술한 Handwriting에 측면에서 살펴보면, 단어를 인식하고, 문장을 인식함에 있어서 앞뒤 문맥간의 관계와, 부분부분의 segmentation의 이해에 대한 어려움이 있으며, 이는 Topology 변화에 대응이 어렵다는 점과 함께, 공존하는 문제점이다. 직관적으로, 이해하고자 하면, Isolated Character Recognition의 측면에서, 대표적인 2가지 문제점이 존재한다. 
 첫번째로, 전형적인 입력 이미지들은 사이즈가 크고, 100개 정도의 Unit이 첫번째 Layer에 속해 있으며, 이미 수천개의 가중치를 가지고 있기 때문에, 그만큼의 큰 크기의 훈련 Set을 가지게 된다. 그만큼 이전에 CNN 보다는 더 넓은 범위의 학습이 가능하지만, 많은 Memory를 할당하여야 한다는 단점이 존재한다. 
 두번째로, Fully-Connected 구조의 비효율성이 하면, 입력의 Topology가 전부 무시됨을 의미한다. 이에 직관적 의미는, 인풋의 변수의 경우 결과의 훈련에 영향을 주지 않은채 나타낼수 있지만, 이미지의 경우 강력한 2D의 Local Structure를 갖고 있기 때문에, 입력 변수로 하여금 topology변화에 대응이 어렵다고 볼수 있다. 
	Motivation
-위에서 언급한 두가지이 문제점을 해결하기 위해 합성곱 네트워크적 측면에서 Shift, Scale 그리고 distortion invariance의 정도를 확실히 하기위해, 3개의 구조적 아이디어들을 접목시켰으며, 이는 각각 Local receptive fields, shared weights 그리고 spatial or temporal sub-sampling이다. 이렇게 처음 만들어진 개념이 LeNet-5의 전신인 LeNet-1이다. 또한 위에 언급한 바와 같은 문제점이 발생하기에 기존에 사용하는 CNN에서 LeNet이라는 알고리즘을 사용하는 이유이기도 하다.
	Preliminaries
-뒤에 LeNet-5의 구조에 대한 연구를 진행 하기전에, 알아두어야 할 부분이 몇 개 존재한다. 우선 Symmetry breaking(대칭성 파괴)이다. 구조에 대한 부분을 설명할때, 언급할 내용이지만, 간략하게 설명을하자면, S2에서 C3로 넘어가는(Fig.1) 부분을 참고하게 되면, 기존에 신경망에서 사용되던 Fully-Connect가 아닌 Table1을 기반으로 한, 연결성을 추구하게 된다. 이러한 이유에는 Twofold 즉 겹쳐지는 경우가 발생하기 때문이다. Dot Prod.를 통해 행렬식이 계산됨에 있어, Symmetry의 조건이 발생할 경우, 대칭성에 의해, 동일가중치가 더해져, 학습이 더디어지는 효과를 낳을 수 있기 때문이다.
 또한, 아래 LeNet구조 마지막 Output 부분을 보면 알수 있듯이, 기존 알고리즘과 같이 Softmax를 활용한 classifier가 아닌, Gaussian Connection에 의한 분류기가 설정되어 있음을 알수 있다. 이는 Euclidian Radial Basis function units.즉 RBF라 하는 함수에 의해 정의됨을 알수 있고, 이 RBF는 입력과 가중치 벡터를 행렬 곱셈 하는 대신 각 뉴런에서 입력 벡터와 가중치 벡터사이의 유클리드 거리를 출력한다. 출력은 이미지가 얼마나 특정숫자 클래스에 속하는지 측정한다. 
y_i=∑_j▒〖(x_j-w_ij)〗^2 
위 식에서 주의깊게 보아야 할점은 parameter vector w_ij의 값이 -1 또는 +1 이라는 점이다. 이러한 이유에는 문자에 유사성에 따른 오류가 발생함을 막기위해 ASCII코드를 활용하는데, 이때의 코드활용을 위해, -1또는 +1의 값이 선택되어진다. 실질적으로 +1과-1은 sigmoid 곡선의 최고점이며, 이는 F6의 값으로 하여금 비선형성을 유지시키는 범위내에서 작용할수 있게 한다.
	Architecture of LeNet-5
 
Figure 1. Architectural Process of LeNet-5

-구조적 과정:
<Input>
	글자인식을 위한 알고리즘 설계를 한다 할때, Gray scale의 32x32x1의 이미지를 입력으로 넣어준다. 
<Input to C1>
	5x5 크기의 필터 6개를 사용하여, 28x28x6의 Feature Map을 형성해준다. 
<C1 to S2>
	Average Pooling, 2x2 필터와 Stride 2의 값을 가지고 변환을 진행하여 14x14x6의 Feature Map을 형성한다.
<S2 to C3>
	5x5의 필터 16개를 사용하여, 10x10x16의 Feature Map을 형성한다. 단 이 부분에서 단순히 Fully Connected 되는 것이 아니라, Table 1에서와 같이, 선택적으로 연결한다. 이러한 이유에는 Symmetry를 파괴12하기 위함이 있다. 이 Feature Map에서는 (5x5)x60+16(Bias)=1516개의 Parameter적 경우의 수를 가지고 있다.
 
Table 1
 
※Symmetry를 지양해야 하는 이유: 만약 머신러닝 모델이 똑같이 초기화된 변수에 가중치를 가지게 되면, 모델을 훈련시킴에 있어, 가중하는 것이 어렵거나 심지어 불가능 할 수도 있다. 이를 Symmetry라 하며, 반드시 지양하여야 한다.
<C3 to S4>
	Average Pooling, 2x2 필터와 Stride 2의 값을 가지고 변환을 진행하여 14x14x6의 Feature Map을 형성한다. 그후 1x1x120의 형상으로 변환시켜 준다.
	120에서 84로 Fully-Connected 한 후, 10개의 class로 RBF(Euclidian Radial basic Function)을 사용하여, 입력과 가중치 간에 거리를 측정하여 이 값을 기준으로 클래스를 분류하게 된다.


층	종류	특성 맵	크기	커널크기	스트라이드	활성화 함수
입력	입력	1	32X32	-	-	-
C1	합성곱	6	28X28	5X5	1	tanh
S2	평균 풀링	6	14X14	2X2	2	tanh
C3	합성곱	16	10X10	5X5	1	tanh
S4	평균 풀링	16	2X2	2X2	2	tanh
C5	합성곱	120	5X5	5X5	1	tanh
F6	완전 연결	-	-	-	-	tanh
출력	완전 연결	-	-	-	-	RBF

Table 2

	Result
 
Figure 2. Size-normalized examples from the MNIST database

 	 	 	 			
Figure 3. E.R(%)/Training set Iteration & E.R(%)/Training set size of LeNet-5
And Deeper Neural learning rate results

 
Figure 4. Relationship between depth of learning volume and error

-LeNet 연구에 대한 기본 표본 조사는 NIST에서 조사된것으로, LeNet-5에서 사용된 표본들은 Modifiy가 이뤄진 MNIST이며, Fig1.에서 볼수 있다 싶이, 20x20의 pixel box로 normalize되어있다. LeNet-5의 여러 버전들이 규칙적인 MNIST에 기반을 둔 data를 기준으로, 20번의 반복학습을 통해 학습되었으며, 이를통한 일률적 통계자료가 Fig 1이라 할수 있다. 보통 19번째의 학습이 반복되어지면 오류률이 0.35%까지로 떨어진다고 한다. 보통 이러한 방식으로 over-training이 될경우 최저점에 오류률을 지나 발산하는 경향을 보이는데 본 연구진들의 연구에선 이와 같은 형상이 보이지 않았고, 그말인즉, 최적점의 가중치가 정착하지 않고, 목표표본의 주변을 진동함을 의미하며, 이는 결국 넓은 범위의 최솟값을 지향하는 결과를 초래하게 되며, 이렇게 될시, 일반화에 오류에 있어, 이득적이게 되는 넓은 엔트로피 parameter distribution의 해결책이라 볼수 있다. 또한 이를 기반으로 Figure3과 같은 결과를 도출해 낼 수 있으며, Figure3.의 경우 60,000가지의 훈련 set를 지나, 학습하는 training set의 표본을 보여주는 그래프로 학습량이 많아짐에 따라 수렴하는 모습을 보이고 있다. 학습의 횟수가 많아지고 오류률의 정도가 줄어듬에 따라, Fig3의 세번째 그림과 같은 여러 형상의 유사성에서도 분류를 잘할수 있게 되었다. 이에 반해, 4번째 그림처럼, 형상에 유사성에 대한 학습이 온전치 못하거나 사람의 눈으로도 식별하기 어려운 경우에는, 오류가 발생함을 알수있어, 훈련의 범위와 반복횟수의 중요성이 드러나게 된다. 마지막 Figure 4의 경우 LeNet의 여러종류에 따른 Error rate를 나타낸 것으로, 기존 Neural Network에서 갓 넘어온, LeNet-1에 비해 LeNet-5, 3개가 겹쳐져서 합성곱으로 표현되는 [dist]Boosted LeNet-4의 경우가 확연히 error rate가 줄어든 것을 확인할수 있다.  
